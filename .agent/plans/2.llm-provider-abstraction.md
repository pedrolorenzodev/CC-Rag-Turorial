# Module 2: LLM Provider Abstraction Layer

**Complexity: ⚠️ Medium** - Clear migration path, same SDK for all providers

## Overview

Migrate from OpenAI Responses API to Chat Completions API with a provider abstraction layer supporting OpenRouter, Ollama, LM Studio, and direct OpenAI.

**Key insight**: All target providers are OpenAI-compatible and use the same `openai` Python SDK with different `base_url` and `api_key` settings. No separate provider classes needed.

---

## Architecture Decision

**Single client with configuration-based provider switching** (not separate classes per provider)

```
Provider       | Base URL                         | API Key
---------------|----------------------------------|---------------------------
OpenAI         | https://api.openai.com/v1        | OPENAI_API_KEY
OpenRouter     | https://openrouter.ai/api/v1     | OPENROUTER_API_KEY
Ollama         | http://localhost:11434/v1        | "ollama" (dummy)
LM Studio      | http://localhost:1234/v1         | "lm-studio" (dummy)
```

---

## File Changes

### New Files

```
backend/app/
├── core/
│   └── config.py              # Centralized Pydantic settings
└── services/
    └── llm/
        ├── __init__.py        # Exports
        ├── client.py          # Unified LLM client
        ├── config.py          # Provider configurations
        └── types.py           # Message types
```

### Modified Files

- `backend/app/routers/chat.py` - Use new LLM service, Chat Completions streaming
- `backend/app/models/thread.py` - Mark `openai_thread_id` as optional/deprecated
- `backend/.env.example` - Add provider configuration variables

### Deprecated/Removed

- `backend/app/services/openai_service.py` - Replace with `llm/` module

---

## Execution Phases

Tasks are organized into phases that can be executed in parallel within each phase.

```
Phase 1 (Parallel)     Phase 2 (Parallel)     Phase 3            Phase 4            Phase 5 (Parallel)
┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐
│ 1a. config.py   │───>│ 2a. llm/config  │───>│ 3. LLM      │───>│ 4. Update   │───>│ 5a. Delete      │
│ 1b. .env.example│    │ 2b. llm/types   │    │    client   │    │    chat.py  │    │     old service │
│ 1c. thread.py   │    │ 2c. llm/__init__│    └─────────────┘    └─────────────┘    │ 5b. langsmith   │
└─────────────────┘    └─────────────────┘                                          └─────────────────┘
```

---

## Implementation Tasks

### Phase 1: Foundation (Parallel)

#### Task 1a: Core Configuration (`backend/app/core/config.py`)

Create Pydantic settings class:

```python
from pydantic_settings import BaseSettings
from typing import Literal, Optional

class Settings(BaseSettings):
    # Provider selection
    llm_provider: Literal["openai", "openrouter", "ollama", "lmstudio"] = "openrouter"
    llm_model: str = "anthropic/claude-3.5-sonnet"

    # API keys (only active provider's key required)
    openai_api_key: Optional[str] = None
    openrouter_api_key: Optional[str] = None

    # Local provider URLs (overridable)
    ollama_base_url: str = "http://localhost:11434/v1"
    lmstudio_base_url: str = "http://localhost:1234/v1"

    # OpenRouter extras
    openrouter_site_url: Optional[str] = None
    openrouter_app_name: Optional[str] = None

    # Observability
    langsmith_enabled: bool = True
    langsmith_api_key: Optional[str] = None
    langsmith_project: str = "rag-masterclass"

    # Supabase (existing)
    supabase_url: str
    supabase_anon_key: str
    supabase_service_role_key: str

    class Config:
        env_file = ".env"
        extra = "ignore"

settings = Settings()
```

**Validation**: Import settings in Python REPL, verify env loading

#### Task 1b: Update Environment Config (`backend/.env.example`)

```bash
# Supabase (unchanged)
SUPABASE_URL=
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=

# LLM Provider Configuration
LLM_PROVIDER=openrouter          # openai | openrouter | ollama | lmstudio
LLM_MODEL=anthropic/claude-3.5-sonnet

# Provider API Keys (only active provider required)
OPENAI_API_KEY=
OPENROUTER_API_KEY=

# Local Provider URLs (optional overrides)
OLLAMA_BASE_URL=http://localhost:11434/v1
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# OpenRouter Extras (optional)
OPENROUTER_SITE_URL=
OPENROUTER_APP_NAME=

# Observability
LANGSMITH_ENABLED=true
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=rag-masterclass
```

#### Task 1c: Update Thread Model (`backend/app/models/thread.py`)

Mark `openai_thread_id` as deprecated with comment:

```python
# Deprecated: Was used for Responses API conversation continuity.
# Module 2+ manages chat history directly via messages table.
openai_thread_id: Optional[str] = None
```

---

### Phase 2: LLM Module Setup (Parallel, requires Phase 1)

#### Task 2a: Provider Configuration (`backend/app/services/llm/config.py`)

```python
from dataclasses import dataclass
from typing import Optional, Dict
from app.core.config import settings

@dataclass
class ProviderConfig:
    base_url: str
    api_key: str
    extra_headers: Optional[Dict[str, str]] = None

def get_provider_config() -> ProviderConfig:
    if settings.llm_provider == "openai":
        return ProviderConfig(
            base_url="https://api.openai.com/v1",
            api_key=settings.openai_api_key,
        )
    elif settings.llm_provider == "openrouter":
        headers = {}
        if settings.openrouter_site_url:
            headers["HTTP-Referer"] = settings.openrouter_site_url
        if settings.openrouter_app_name:
            headers["X-Title"] = settings.openrouter_app_name
        return ProviderConfig(
            base_url="https://openrouter.ai/api/v1",
            api_key=settings.openrouter_api_key,
            extra_headers=headers or None,
        )
    elif settings.llm_provider == "ollama":
        return ProviderConfig(
            base_url=settings.ollama_base_url,
            api_key="ollama",
        )
    elif settings.llm_provider == "lmstudio":
        return ProviderConfig(
            base_url=settings.lmstudio_base_url,
            api_key="lm-studio",
        )
```

**Validation**: Unit test each provider config

#### Task 2b: Message Types (`backend/app/services/llm/types.py`)

```python
from pydantic import BaseModel
from typing import Literal

class ChatMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str
```

#### Task 2c: Module Exports (`backend/app/services/llm/__init__.py`)

```python
from app.services.llm.client import stream_chat, get_client
from app.services.llm.types import ChatMessage

__all__ = ["stream_chat", "get_client", "ChatMessage"]
```

---

### Phase 3: LLM Client (Sequential, requires Phase 2)

#### Task 3: LLM Client (`backend/app/services/llm/client.py`)

```python
from typing import AsyncGenerator, List
import openai
from langsmith.wrappers import wrap_openai
from langsmith import traceable

from app.core.config import settings
from app.services.llm.config import get_provider_config
from app.services.llm.types import ChatMessage

_client = None

def get_client() -> openai.OpenAI:
    global _client
    if _client is None:
        config = get_provider_config()
        base_client = openai.OpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
            default_headers=config.extra_headers,
        )
        if settings.langsmith_enabled and settings.langsmith_api_key:
            _client = wrap_openai(base_client)
        else:
            _client = base_client
    return _client

SYSTEM_PROMPT = """You are a helpful AI assistant..."""

@traceable(name="chat_completion")
def stream_chat(
    messages: List[ChatMessage],
) -> AsyncGenerator[str, None]:
    """Stream chat completion. Returns content chunks."""
    client = get_client()

    full_messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        *[{"role": m.role, "content": m.content} for m in messages],
    ]

    stream = client.chat.completions.create(
        model=settings.llm_model,
        messages=full_messages,
        stream=True,
    )

    for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```

**Validation**: Test streaming with each provider

---

### Phase 4: Integration (Sequential, requires Phase 3)

#### Task 4: Update Chat Router (`backend/app/routers/chat.py`)

Key changes:
1. Import from new `llm` module
2. Remove `previous_response_id` logic (no longer needed)
3. Remove `vector_store_id` handling (defer to later RAG implementation)
4. Update streaming to Chat Completions format

```python
# Before (Responses API)
async for chunk, rid in stream_response(
    messages=messages,
    previous_response_id=thread.get("openai_thread_id"),
    vector_store_id=vector_store_id,
):

# After (Chat Completions API)
for chunk in stream_chat(messages=chat_messages):
    full_response += chunk
    encoded_chunk = chunk.replace("\n", "\\n").replace("\r", "\\r")
    yield {"event": "message", "data": encoded_chunk}
```

Remove: `openai_thread_id` update logic (lines 104-108 in current chat.py)

**Validation**: Test chat endpoint with SSE, verify streaming works

---

### Phase 5: Cleanup (Parallel, requires Phase 4)

#### Task 5a: Delete Old Service

Delete `backend/app/services/openai_service.py` (no longer needed)

#### Task 5b: Simplify LangSmith Service

Review `backend/app/services/langsmith_service.py` - may be able to remove if all tracing is handled via `wrap_openai()` in the new LLM client.

---

## Streaming Format Change

| Responses API                    | Chat Completions API                |
|----------------------------------|-------------------------------------|
| `response.output_text.delta`     | `chunk.choices[0].delta.content`    |
| `response.completed`             | `chunk.choices[0].finish_reason`    |
| `previous_response_id`           | Send full message history           |

---

## Verification

1. **Unit test**: Provider config returns correct values for each provider
2. **Integration test**: Stream chat with OpenRouter (primary)
3. **Integration test**: Stream chat with Ollama (local)
4. **E2E test**: Full chat flow in browser
5. **LangSmith**: Verify traces appear in dashboard

---

## Decision: RAG Handling

**Remove OpenAI vector store logic for now.** The current `file_search` tool and `vector_store_id` handling is OpenAI-specific. We'll implement provider-agnostic RAG in a later module using Supabase pgvector.

This means:
- Remove `vector_store_id` parameter from chat flow
- Remove `create_vector_store()` and `upload_file_to_vector_store()` functions
- Keep `vector_store_id` column in threads table (no migration needed, just unused)
